\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\begin{document}

\title{Accelerating Autonomous Vehicles with Unity for Training and Test Optimization}

% For details on assignment: https://sdsu.instructure.com/courses/163996/assignments/1439561

\author{
\IEEEauthorblockN{Ethan Mullen}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{San Diego State University}\\
San Diego, USA \\
emullen9665@sdsu.edu}
\and
\IEEEauthorblockN{Kyle Krick}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{San Diego State University}\\
San Diego, USA \\
kkrick0067@sdsu.edu}
\and
\IEEEauthorblockN{Dominic Griffith}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{San Diego State University}\\
San Diego, USA \\
dgriffth6227@sdsu.edu}
\and
\IEEEauthorblockN{Tanishq Patil}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{San Diego State University}\\
San Diego, USA \\
tpatil5240@sdsu.edu}
}
\maketitle

\begin{abstract}
High-quality simulations with realistic sensor data in Unity can be used to accelerate the training and testing of Autonomous Vehicles for edge-case scenarios.
\end{abstract}

\begin{IEEEkeywords}
Artificial Intelligence, Autonomous Vehicles, Machine Learning.
\end{IEEEkeywords}

\section{Introduction}
Autonomous vehicle (AV) development and safety testing is difficult and time consuming due to their underlying machine learning (ML) models\cite{koopman2016}.
The ML models powering AVs need more data on edge cases like severe weather events, unpredictable behavior from other drivers, and ambiguous traffic situations.
We wanted to address the issues of limited data for edge cases scenarios and time required for training and testing AVs.
We delved into the potential of Unityâ€™s video game engine and its ML-Agents Toolkit to address these challenges.

\section{Work performed}
\subsection{The Environment}
In order to train our autonomous vehicle virtually to be used in the real world, we focused on creating an environment that was as realistic as possible. The most optimal solution would be a 3D scan of a real-life environment like the intersection at College Avenue and Canyon Crest Drive at the bottom of campus; however, this requires expensive equipment that we don't have access to. The next best option would be to hire professional 3D artists to recreate this environment virtually, which would be incredibly time-consuming. Both of these solutions require a large budget that we can't achieve.\par
To match our needs with our limited budget, we found that purchasing an environment that was as realistic as possible was the best solution- even if it wasn't based on a real location. We found the New York Streets Modular City environment on the Unity Asset Store to satisfy this \cite{newYorkStreets2024}. Theoretically, if the environment provides accurate enough data for the cameras and sensors to train on, the virtual model should translate to real life.\par
Once we acquired the environment we wanted to use, it needed to be prepared to become a better training environment for the vehicle. The first thing that we needed to fix was the roads: the three directions of travel had inconsistent center lines, with the non-primary roads having dotted center lines and the primary road having a double solid line. This insinuates that you could pass over the line on the non-primary roads, which would put the vehicle in the wrong direction of traffic; the non-primary roads' center lines were replaced with double solid lines for consistency. At the intersection, there were no stopping lines indicating where to stop for the traffic lights, this would be a problem as the autonomous vehicle wouldn't know where to stop. We added stopping lines and arrows indicating the directions you could turn in the intersection safely for clarity. The last thing that needed to be added to the base environment was functioning traffic lights \cite{trafficLights2024}; these were cheap to purchase which was perfect for our use case. However, the traffic lights were static; custom scripts were written to change the material of each traffic light indicator (red, yellow, and green) to make them emissive depending on what signal we wanted to show. Also, for the clarity of the autonomous vehicle, shades were added over each light to ensure it was clear which light was active.\par
After the scene was configured properly for our use case, we needed to add rewards/punishments for our model to train with. In Unity, one can set up ``Box Colliders" with the ``trigger" tag enabled so the autonomous vehicle can pass through without stopping in its tracks. This is useful for us as there are callbacks in the script for the autonomous vehicle to indicate when it hits one of these box colliders (we will be referring to them as TrainingTriggers from now on). On each TrainingTrigger, we can set the float value that we want to apply to the model as it passes through. We used TrainingTriggers on the entirety of the scene to indicate to the model what we want it to do. TrainingTriggers were placed:
\begin{enumerate}
    \item On curbs with rewards of -15 to indicate that the autonomous vehicle should not jump the curb.
    \item On the center lines with rewards of -10 to indicate the center lines should not be passed over.
    \item On dotted lines with rewards of -5 to indicate the model should stay in the current lane, and only pass over when necessary.
    \item At each lane's entrance to the intersection, with variable rewards tied to its respective traffic light's state to ensure that the autonomous vehicle knows when it can enter the intersection. Values were -12, -2, and 5 for red, yellow, and green respectively.
\end{enumerate}\par
With the TrainingTriggers in place, we needed a way to consistently reward the model as it passes through the environment. The solution we found for this we called a TrainingPath, a series of TrainingTriggers placed in the middle of each lane on the route that we want the model to take. Each TrainingTrigger along this path is worth 4 reward. The first TrainingTrigger's position and rotation are applied to the autonomous vehicle to start the episode, and once the autonomous vehicle reaches the last TrainingTrigger the episode is ended successfully. For each TrainingPath, a series of instructions are included for the model to read, such as continue\_straight, lane\_change\_left, and intersection\_turn\_right.

\subsection{The Car}
We built our own car in Unity and gave it basic properties, such as realistic mass, acceleration, turning radius, and breaking power. It is important to have the right configuration of these properties because the Unity physics engine relies on them for simulating the car's motion. 
\par
We built the car incrementally, first we built a keyboard-controlled version of the car. You can drive this version of the car using keyboard input, forward and down arrows for forward or reverse motion, arrow keys for turning, and space bar for breaks. To accomplish this task; we used the free ARCADE car package for the body of the car, to this body attached a RigidBody component. A RigidBody in Unity is a component that gives a GameObject, our car's body in this case, properties of physics. Then we added a box collider and fit it to the car's body excluding the wheels. The box collider detects collisions and is also used to detect the tag of objects.
\par

In order to give the car motion based on keyboard inputs, we added four wheel collider components and fit it to each wheel. A wheel collider has multiple properties that can be used to make the car as realistic as possible. Using wheel collider we can set the car's turning radius, traction and suspension.
Once the car's body was complete, we created a car controller script to control the car using keyboard inputs. The car controller script uses the front wheels to make the car move. 
\par

After attaching the car controller script to the car, we performed multiple motion tests to set the above mentioned RigidBody and the wheel collider properties to make the car as realistic as possible. Once we had a realistic car model that we could control with keyboard inputs we fitted the car with two types of sensors:
\begin{enumerate}
    \item \textbf{3D Ray Perception Sensor:} This sensor is a part of the mlagents library, it casts laser rays in multiple directions to detect the distance of objects. We used 1 such sensors, it casts 3 rays in each direction.
    \item \textbf{Camera sensor:} These sensors are the car's eyes. We have three forward facing camera sensors to give the car a wide forward and peripheral view.
\end{enumerate}

With the sensors in place and a working car, we can now use this car and train it in the environment mentioned above.

\subsection{ML Agents Configuration}

\subsection{Traffic Simulation}
An AV should be able to navigate among other vehicles in traffic. 
In order to meet this need, we evaluated SUMO which is an open-source traffic simulator\cite{alvarezlopez2018}.
We used SUMO to design a traffic simulation that aligned with the training environment that we developed.
The SUMO simulation is comprised of nodes, edges and preplanned routes for vehicles to travel through the road network.
With this traffic simulation working, we evaluated software for integrating SUMO and Unity.
We focused on Sumonity which is an open-source software project attempting to run co-simulations between Unity and SUMO\cite{pechinger2024sumonity}.
However, Sumonity was lacking technical documentation and was too tightly coupled to its original project implementation. 
Although we reached out to the team developing the software, we were not able to get assistance with reusing Sumonity.
Given these technical hurdles, we decided to pivot our remaining time and effort to attempt to produce a functioning AV model.

\subsection{Accelerating Training}
Training a ML model with PyTorch is a computationally expensive and time consuming task.
In order to accelerate our training, we turned to the Technology Infrastructure for Data Exploration (TIDE) cluster for access to GPUs\cite{tidesdsu}.
TIDE is a Kubernetes cluster and requires that code be encapsulated in a software container prior to scheduling on the cluster.
We developed a custom software container that had our ML-Agents python environment and Unity pre-installed.
Our container leveraged Selkies, an open-source GPU accelerated desktop streaming solution, to run and view our ML training in real-time\cite{selkiestreamer}.
With this software container we were able to access GPUs on TIDE and we could run up to six instances of our training environment within one Unity session, both of which allowed us to speed up our ML-Agents training time.
We believe that parallelization of training environments, access to GPUs and more advanced distributed computing techniques can aid in accelerating model training.

\section{Results}
We evaluated our AV models based on their mean reward and watching their performance in driving our model car in real-time.
We expected a successful model to gain a mean reward near 16.0 points, since our TrainingPaths consisted of four TrainingTriggers each rewarding 4.0 points.
We completed three full training sessions and produced models that earned mean rewards of 4.0, -0.25 and 0.0 points.
Although we did produce one model with a positive mean reward, watching our models in real-time revealed that none of the models were very good at driving our car through the training environment.
Two of the models displayed a crab-like driving behavior where they turned their wheels to move laterally, thus hitting the curb repeatedly.
Our best model, with 4.0 points, grabbed the first positive reward and then moved back and forth in-place until the training session concluded.
We made adjustments in between training runs such as enhancing the number of 3D Ray Perception Sensors, the detection distance of the sensors and the camera refresh rate, but none of these adjustments made a profound improvement.

\section{Conclusion}
We explored the capabilities of Unity to solve the issues of edge case scenarios and accelerating training time for AVs.
Our goals were ambitious, so we started small by developing a single training environment that was configured for reinforcement learning with object labels, route paths and positive and negative rewards.
Unity's ML-Agents Toolkit allowed us to train a ML model to drive our model car, which was equipped with sensors and cameras.
In order to accelerate our training time, we developed a container to leverage the TIDE cluster for access to GPUs.
Although we developed a traffic simulation in SUMO, we did not integrate it into our training environment, choosing instead to devote more resources to training our model.
Ultimately, the ML model that we were able to produce was not successful in learning to navigate our training environment.
Without a basic working model, we were not able to advance our efforts to address edge case scenarios in AV training.

\par
You can see details on our project at: https://github.com/east-01/AAUTO

\section{Acknowledgments}
\subsection{Ethan Mullen}
Anything related to the environment: Purchased base environment and converted to a training environment including the addition of fully functional traffic lights, customization of roads to increase consistency and match real-world roads more accurately, the addition of triggers with variable rewards to train model, the addition of training paths and path instructions for the model.
\subsection{Tanishq Patil}
Building the keyboard-controlled car, integrating the necessary sensors, and developing the car controller script.
\subsection{Kyle Krick}
Developed the SUMO simulation based on the training environment, investigated the use of Sumonity  for co-simulation with Unity, developed a Docker container for the project to be able to run on the TIDE cluster, and oversaw the model training sessions.

\bibliography{references}
\bibliographystyle{IEEEtran}

\end{document}
