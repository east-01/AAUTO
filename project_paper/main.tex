\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\begin{document}

\title{Accelerating Autonomous Vehicles with Unity for Training and Test Optimization}

% For details on assignment: https://sdsu.instructure.com/courses/163996/assignments/1439561

\author{
\IEEEauthorblockN{Ethan Mullen}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{San Diego State University}\\
San Diego, USA \\
emullen9665@sdsu.edu}
\and
\IEEEauthorblockN{Kyle Krick}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{San Diego State University}\\
San Diego, USA \\
kkrick0067@sdsu.edu}
\and
\IEEEauthorblockN{Dominic Griffith}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{San Diego State University}\\
San Diego, USA \\
dgriffth6227@sdsu.edu}
\and
\IEEEauthorblockN{Tanishq Patil}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{San Diego State University}\\
San Diego, USA \\
tpatil5240@sdsu.edu}
}
\maketitle

\begin{abstract}
High-quality simulations with realistic sensor data in Unity can be used to accelerate the training and testing of Autonomous Vehicles for edge-case scenarios.
\end{abstract}

\begin{IEEEkeywords}
Artificial Intelligence, Autonomous Vehicles, Machine Learning.
\end{IEEEkeywords}

\section{Introduction}
% In this project, we are exploring the capabilities of the video game engine Unity for training and testing machine learning (ML) models for autonomous vehicles, referred to as AVs. My contribution will be creating virtual environments and measuring their accuracy; accuracy refers to how close the virtual environment can reflect the real world in the eyes of the ML model. These virtual environments must closely reflect their real counterparts to train each ML model properly.

\section{Work performed}
\subsection{The Environment}
In order to train our autonomous vehicle virtually to be used in the real world, we focused on creating an environment that was as realistic as possible. The most optimal solution would be a 3D scan of a real-life environment like the intersection at College Avenue and Canyon Crest Drive at the bottom of campus; however, this requires expensive equipment that we don't have access to. The next best option would be to hire professional 3D artists to recreate this environment virtually, which would be incredibly time-consuming. Both of these solutions require a large budget that we can't achieve.\par
To match our needs with our limited budget, we found that purchasing an environment that was as realistic as possible was the best solution- even if it wasn't based on a real location. We found the New York Streets Modular City environment on the Unity Asset Store to satisfy this \cite{newYorkStreets2024}. Theoretically, if the environment provides accurate enough data for the cameras and sensors to train on, the virtual model should translate to real life.\par
Once we acquired the environment we wanted to use, it needed to be prepared to become a better training environment for the vehicle. The first thing that we needed to fix was the roads: the three directions of travel had inconsistent center lines, with the non-primary roads having dotted center lines and the primary road having a double solid line. This insinuates that you could pass over the line on the non-primary roads, which would put the vehicle in the wrong direction of traffic; the non-primary roads' center lines were replaced with double solid lines for consistency. At the intersection, there were no stopping lines indicating where to stop for the traffic lights, this would be a problem as the autonomous vehicle wouldn't know where to stop. We added stopping lines and arrows indicating the directions you could turn in the intersection safely for clarity. The last thing that needed to be added to the base environment was functioning traffic lights \cite{trafficLights2024}; these were cheap to purchase which was perfect for our use case. However, the traffic lights were static; custom scripts were written to change the material of each traffic light indicator (red, yellow, and green) to make them emissive depending on what signal we wanted to show. Also, for the clarity of the autonomous vehicle, shades were added over each light to ensure it was clear which light was active.\par
After the scene was configured properly for our use case, we needed to add rewards/punishments for our model to train with. In Unity, one can set up "Box Colliders" with the "trigger" tag enabled so the autonomous vehicle can pass through without stopping in its tracks. This is useful for us as there are callbacks in the script for the autonomous vehicle to indicate when it hits one of these box colliders (we will be referring to them as TrainingTriggers from now on). On each TrainingTrigger, we can set the float value that we want to apply to the model as it passes through. We used TrainingTriggers on the entirety of the scene to indicate to the model what we want it to do. TrainingTriggers were placed:
\begin{enumerate}
    \item On curbs with rewards of -15 to indicate that the autonomous vehicle should not jump the curb.
    \item On the center lines with rewards of -10 to indicate the center lines should not be passed over.
    \item On dotted lines with rewards of -5 to indicate the model should stay in the current lane, and only pass over when necessary.
    \item At each lane's entrance to the intersection, with variable rewards tied to its respective traffic light's state to ensure that the autonomous vehicle knows when it can enter the intersection. Values were -12, -2, and 5 for red, yellow, and green respectively.
\end{enumerate}\par
With the TrainingTriggers in place, we needed a way to consistently reward the model as it passes through the environment. The solution we found for this we called a TrainingPath, a series of TrainingTriggers placed in the middle of each lane on the route that we want the model to take. Each TrainingTrigger along this path is worth 5 reward. The first TrainingTrigger's position and rotation are applied to the autonomous vehicle to start the episode, and once the autonomous vehicle reaches the last TrainingTrigger the episode is ended successfully. For each TrainingPath, a series of instructions are included for the model to read, such as continue\_straight, lane\_change\_left, and intersection\_turn\_right.

\par
You can see details on our project at: https://github.com/east-01/AAUTO

\section{Acknowledgements}
\subsection{Ethan Mullen}
Anything related to the environment: Purchased base environment and converted to a training environment including the addition of fully functional traffic lights, customization of roads to increase consistency and match real-world roads more accurately, the addition of triggers with variable rewards to train model, the addition of training paths and path instructions for the model.

\bibliography{references}
\bibliographystyle{IEEEtran}

\end{document}
